\documentclass[a4paper,UKenglish]{darts-v2021}
%This is a template for producing DARTS artifact descriptions. 
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
% for section-numbered lemmas etc., use "numberwithinsect"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"
 
\usepackage{microtype}%if unwanted, comment out or use option "draft"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

% \nolinenumbers to disable line numbers

\bibliographystyle{plainurl}% the mandatory bibstyle

% Commands for artifact descriptions
% Written by Camil Demetrescu and Erik Ernst
% April 8, 2014

\newenvironment{scope}{\section{Scope}}{}
\newenvironment{content}{\section{Content}}{}
\newenvironment{getting}{\section{Getting the artifact} The artifact 
endorsed by the Artifact Evaluation Committee is available free of 
charge on the Dagstuhl Research Online Publication Server (DROPS).}{}
\newenvironment{platforms}{\section{Tested platforms}}{}
\newcommand{\license}[1]{{\section{License}#1}}
\newcommand{\mdsum}[1]{{\section{MD5 sum of the artifact}#1}}
\newcommand{\artifactsize}[1]{{\section{Size of the artifact}#1}}

\newcommand{\AD}[1]{\textcolor{red}{AD: #1}}

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{WebGlitch: A Randomised Testing Tool for the WebGPU API (Artifact)} %TODO Please add

% \titlerunning{A Sample DARTS Research Description (Artifact)} %optional, in case that the title is too long; the running title should fit into the top page column

% ARTIFACT: Authors may not be exactly the same as the related scholarly paper, e.g., you may want to include authors who contributed to the preparation of the artifact, but not to the companion paper
\author{Matthew K. L. Wong}{Department of Computing, Imperial College London, United Kingdom}{kwok.wong19@imperial.ac.uk}{https://orcid.org/0009-0004-2776-5430}{}
%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional

\author{Alastair F. Donaldson}{Department of Computing, Imperial College London, United Kingdom}{alastair.donaldson@imperial.ac.uk}{https://orcid.org/0000-0002-7448-7961}{}

\authorrunning{M.\,K.\,L. Wong and A.\,F. Donaldson}%TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Matthew K. L. Wong and Alastair F. Donaldson}%TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[500]{Software and its engineering~Object oriented languages}


\keywords{Fuzzing, WebGPU, WGSL, API, shaders, artifact} 

%TODO Please provide information to the related scholarly article
\RelatedArticle{Matthew K. L. Wong and Alastair F. Donaldson, ``WebGlitch: A Randomised Testing Tool for the WebGPU API'', in Proceedings of the 30th Conference on Very Important Topics (CVIT 2016), LIPIcs, Vol.~0, pp.~0:1--0:2, 2016.\newline \url{https://doi.org/10.4230/LIPIcs.xxx.xxx.xxx}}

% \acknowledgements{I want to thank \dots}%optional

\funding{This work was supported by EPSRC grant EP/R006865/1 and gift funding from Google.}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\nolinenumbers %uncomment to disable line numbering

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Volume{3}
\Issue{2}
\RelatedConference{39th European Conference on Object-Oriented Programming (ECOOP 2025), June 30--July 2, 2025, Bergen, Norway}

\RelatedArticle{Matthew K. L. Wong and Alastair F. Donaldson, ``WebGlitch: A Randomised Testing Tool for the WebGPU API'', in 39th European Conference on Object-Oriented Programming (ECOOP 2025), LIPIcs, Vol.~333, pp.~[from]--[to], 2025.
\newline\url{https://doi.org/[doi-of-conference-paper]}}

\Article{14}
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
We present the software artifact that accompanies our paper on WebGlitch, a new tool for fuzz testing WebGPU implementations, along with detailed instructions for its use.
The artifact consists of a Docker image containing the complete setup to run WebGlitch on two WebGPU implementations, Dawn and wgpu, via Node.js and Deno, respectively.
This image allows users to reproduce the throughput and coverage experiments described in the paper, and provides a playground for generating WebGPU programs with WebGlitch and testing different WebGPU implementations. 
We also include the full source code for WebGlitch for transparency.
We hope this artifact will support future research in testing the WebGPU ecosystem and API fuzzing more broadly.
 \end{abstract}

% ARTIFACT: please stick to the structure of 7 sections provided below

% ARTIFACT: section on the scope of the artifact (what claims of the paper are intended to be backed by this artifact?)
\begin{scope}
The artifact provides 1) the source code for WebGlitch and 2) a Docker image with all required dependencies and configurations for running WebGlitch, executing the programs it generates against Dawn~\cite{dawn} (via Node.js) and wgpu~\cite{wgpu} (via Deno), and evaluating WebGlitch's performance. 
It includes scripts to reproduce the following averaged results from the paper:
\begin{itemize}
    \item WebGlitch takes an average of 0.675 seconds to generate a WebGPU program, while executing the generated program takes much longer, averaging 2.932 seconds (Section 3.7). These results, of course, depend on the hardware used to run the artifact.
    \item The absolute coverage of Dawn by WebGlitch and wg-fuzz~\cite{wgfuzz} is similar, averaging 13.52\% and 15.71\%, respectively, while the WebGPU Conformance Test Suite (CTS)~\cite{cts} achieves 28.00\% coverage (Section 5). 
    \item WebGlitch covers approximately 1,530 lines of Dawn not covered by wg-fuzz, while wg-fuzz covers around 5,180 lines of Dawn not reached by WebGlitch (Section 5). 
    \item WebGlitch is able to cover lines of Dawn missed by the CTS (Section 5).
\end{itemize}
Please note that program generation is randomised, so throughput and coverage results can vary from those reported.
Additionally, some CTS tests exhibit flakiness, which may affect also coverage measurements.
\end{scope}

% ARTIFACT: section on the contents of the artifact (code, data, etc.)
\begin{content}
The artifact includes the WebGlitch source code, along with a Docker build file (Dockerfile.artifact) used to create the provided Docker image. This image contains the following components:
\begin{itemize}
    \item A pre-built version of WebGlitch (in \texttt{/WebGlitch}). WebGlitch can be built with Gradle 8.9 by running \texttt{grade build}.
    \item The compiled version of wg-fuzz used to measure coverage (in \texttt{/wg-fuzz}).
    \item Dawn (hash 6e033b) compiled with Node bindings and coverage collection enabled (in \texttt{/dawn}). 
    \item Deno v2.2.12, which includes wgpu (in \texttt{/opt/deno}).
    \item The coverage results we collected in our experiments in JSON representation (in /CovCompare). Within /CovCompare are the results from WebGlitch (\texttt{webglitch\_no\_swarm\_formatted.json}), wg-fuzz (\texttt{wg\_fuzz\_no\_swarm\_formatted.json}), and the CTS (\texttt{cts\_api\_formatted.json} and \texttt{cts\_shader\_formatted.json}). Coverage results from the CTS, which include both API and shader tests, were collected separately and then combined.
\end{itemize}
\end{content}

% ARTIFACT: section containing links to sites holding the
% latest version of the code/data, if any
\begin{getting}
Running WebGPU programs requires direct access to a GPU. 
Obtaining GPU access within a virtual machine or container can be problematic. 
Therefore, we recommend using a Linux-based machine with a physical GPU to run the container.
This system should also have Vulkan installed, which can be done with:
\begin{itemize}
    \item \texttt{apt-get install -y vulkan-tools libvulkan1}
\end{itemize}
After installation, running \texttt{vulkaninfo} should list the name of the physical GPU. 
If it only lists a software rasteriser like LLVMpipe, then the physical GPU is not accessible and the artifact may not work.
Since the artifact has been provided as a Docker image and has been tested with an NVIDIA GPU, the NVIDIA container toolkit must also be installed to allow GPU access within the container. Installation instructions are available at \url{https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html}.
The Docker image provided in the artifact can then be decompressed and loaded with the following command:
\begin{itemize}
    \item \texttt{gzip -d -c webglitch\_image.tar.gz | docker load}
\end{itemize}
Alternatively, \texttt{pigz} may be installed and used to speed up decompression:
\begin{itemize}
    \item \texttt{sudo apt install pigz}
    \item \texttt{pigz -d -c webglitch\_image.tar.gz | docker load}
\end{itemize}
The Docker image can also be built by unzipping the provided WebGlitch source code and running the following command within \texttt{/WebGlitch}:
\begin{itemize}
    \item \texttt{docker build -f Dockerfile.artifact -t webglitch\_image .}
\end{itemize}
The Docker image can then be run with:
\begin{itemize}
    \item \texttt{docker run -it webglitch\_image}
\end{itemize}
Basic functionality can be tested with a shortened version of the evaluation tests within the \texttt{/WebGlitch} directory (commands explained in Appendix):
\begin{itemize}
    \item \texttt{python3 time.py -{}-num\_runs 5}
    \item \texttt{python3 coverage.py -{}-num\_tests 5 -{}-fast}
\end{itemize}
\end{getting}
Successful execution should result in throughput and coverage data being output to the console. While running coverage tests, it may indicate some tests have not passed; this is expected as certain CTS tests experience flakiness, and half of the tests generated by the fuzzers intentionally bypass validity checks to exercise the portions of Dawn responsible for validating API calls.
During coverage testing, some tests may be reported as failing; this is expected, as certain CTS tests are known to be flaky, and approximately half of the tests generated by the fuzzers intentionally bypass validity checks.
% ARTIFACT: section specifying the platforms on which the artifact is known to
% work, including requirements beyond the operating system such as large
% amounts of memory or many processor cores
\begin{platforms}
The artifact is known to work on Ubuntu 24.04 with the following hardware configuration: Intel i7-13700K, 64 GB DDR5 RAM (>128 GB recommended), NVIDIA RTX 4080, and 500 GB of available disk space. 
Collecting coverage data for the CTS requires substantial memory; in our experiments, systems with only 32 GB of RAM experienced test failures due to out-of-memory errors.
The container also requires access to the GPU, requiring the installation of the NVIDIA container toolkit, the relevant graphics drivers for the GPU available in the system, and Vulkan (see `Getting the artifact').
The correct graphics drivers must be installed in the container as well; the provided Docker image was built with NVIDIA driver version 550. Running the artifact on a different GPU hardware may require a different driver.
Compatibility with AMD graphics cards and/or other operating systems is not guaranteed. 
\end{platforms}

% ARTIFACT: section specifying the license under which the artifact is
% made available
\license{The artifact is available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.}

% ARTIFACT: section specifying the md5 sum of the artifact master file
% uploaded to the Dagstuhl Research Online Publication Server, enabling 
% downloaders to check that the file is the expected version and suffered 
% no damage during download.
\mdsum{9c244f4aedef985da7430aeea1d51940}

% ARTIFACT: section specifying the size of the artifact master file uploaded
% to the Dagstuhl Research Online Publication Server
\artifactsize{9.25 GiB}

% ARTIFACT: optional appendix
\appendix
\section{Artifact usage and reproducibility}
The throughput analysis results presented in Section 3.7 can be reproduced by running the command \texttt{python3 time.py -{}-num\_runs 1000} from within the \texttt{/WebGlitch} directory.
By default, the script already generates and executes 1,000 WebGPU programs.
Similarly, coverage measurements can be reproduced by running \texttt{python3 coverage.py} in the same directory.
By default, 2,000 programs are generated by both WebGlitch and wg-fuzz, half of which will skip validity checking.
This experiment is then repeated three times.
The total number of programs generated by the fuzzers can be set with \texttt{-{}-num\_tests}.
Running the CTS may be time-consuming. To speed up evaluation, pre-collected WebGPU CTS coverage data can be used by passing the flag \texttt{-{}-fast} when running the command.
Both scripts automatically handle repetition and averaging of results.
Executing both scripts with default arguments can take around 12 hours.
WebGlitch itself can be run independently with \texttt{python3 webglitch.py}. 
% ARTIFACT: include here any additional references, if needed...

%%
% Bibliography


%% Either use bibtex (recommended), 

\bibliography{bibliography}

%% .. or use the thebibliography environment explicitely



\end{document}
